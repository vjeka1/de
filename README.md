- **ETL_project_clean.tar.gz** - архив с ETL-проектом (Docker Compose, NiFi, PostgreSQL, MinIO)
- **de_ddl.sql** - SQL-скрипты для создания структуры данных
- **de_dml.sql** - SQL-скрипты для манипуляции данными
- **de_python_sql.ipynb** - Jupyter Notebook с Python-решением

# Инструкция по проверке ETL-проекта

## 1. Развертывание инфраструктуры

```bash
# Создаем директорию для проекта
mkdir -p ~/etl_solution

# Распаковываем архив в созданную директорию
tar -xzvf ETL_project_clean.tar.gz -C ~/etl_solution

# Переходим в директорию проекта
cd ~/etl_solution

# Делаем скрипт исполняемым
chmod +x deploy_main.sh

# Запускаем скрипт развертывания
./deploy_main.sh
```

## 2. Проверка компонентов ETL-системы

### 2.1. Проверка MinIO (S3)

1. Открываем веб-интерфейс MinIO: [http://0.0.0.0:9011/](http://0.0.0.0:9011/)
2. Вводим учетные данные:
   - Логин: `minioadmin`
   - Пароль: `minioadmin`
3. Проверяем наличие бакета `etl-bucket`
4. Внутри бакета подтверждаем наличие файла CSV (`sample.csv`)

### 2.2. Проверка PostgreSQL

1. Подключаемся к PostgreSQL с помощью любого SQL-клиента (pgAdmin, DBeaver и т.д.):
   - Строка подключения: `jdbc:postgresql://0.0.0.0:5442/etl_demo`
   - Хост: `0.0.0.0` (или `localhost`)
   - Порт: `5442`
   - База данных: `etl_demo`
   - Логин: `postgres`
   - Пароль: `postgres`
2. Проверяем наличие таблицы `public.sample_data`
3. Убеждаемся, что таблица на данный момент пуста

### 2.3. Проверка Apache NiFi и выполнение ETL-процесса

1. Открываем веб-интерфейс NiFi: [http://0.0.0.0:8180/nifi](http://0.0.0.0:8180/nifi)
2. Запускаем созданную группу процессоров (через кнопку "Play" в панели управления)
3. После завершения процесса повторно проверяем таблицу `public.sample_data` в PostgreSQL
4. Подтверждаем, что данные из CSV-файла успешно загружены в таблицу
